import { Callout } from 'nextra/components'

# What are Blocks?

Blocks in Promptfile simplify LLM requests by making them easy to read and understand. They prevent common mistakes and give you more power to create complicated interactions with the LLM.

### Assistant
Use the `<Assistant>{:prompt}` to specify that the content is coming from the model

 - **name** <i>(optional string) </i>— Assign a name to the assistant.
 - **type** <i>(optional enumerated string)</i> — Automatically generated condition for tracking the assistant response type. Currently only `function_call`.

```prompt copy filename="assistantBlock_example.prompt"
<Assistant>
Hello human!
</Assistant>
```

### User

Use the `<User>{:prompt}` block to indicate the text originating from a human user.

 - **name** <i>(optional string)  </i> — Assign a name to the user.

```prompt copy filename="user_example.prompt"
<User>
Hello AI!
</User>

```
### System

Use the `<System>{:prompt}` block to provide instructions to the LLM.

For Anthropic models, the `<System>{:prompt}` block has the same effect as the `<User>{:prompt}` block.


```prompt copy filename="system_example.prompt"
<System>
You are a friendly pirate. Only respond in the style and tone of a friendly pirate.
</System>
```


### Functions

Use the `<Functions>{:prompt}` block to define tools that the LLM is able to use. When the LLM wants to use a function, it will return a response with the name of the function it to use and the arguments to call it with.

A `<Functions>{:prompt}` block has the same JSON structure as defined by OpenAI. Read more about it [here](https://platform.openai.com/docs/guides/gpt/function-calling).

Currently, only `gpt-4-0613` and `gpt-3.5-turbo-0613` models support `<Functions>{:prompt}`.

In the example below, we define a function called `calculator`. It includes a `testValue` that our function (implemented elsewhere in our project) might return.

Since OpenAI does not execute the function with the parameters it returns, it's up to you to run it and return a value if you wish to continue the conversation with the output from that function. In Promptfile, the output of your function running the arguments from the LLM are contained in a `<Function>{:prompt}`. See the example below to see how they work together.

### Function

The `<Function>{:prompt}` block is automatically generated and used to track the results of a function call.
 - **name** <i>(string)</i> The name of the function which was called.

For example, here is a sample Promptfile that declares a function called "calculator".


```prompt copy filename="functions.prompt"
---
model: gpt-3.5-turbo-0613
---

<Functions>
[
  {
    "name": "calculator",
    "description": "Calculates the result of a math expression using JavaScript `eval`.",
    "parameters": {
      "type": "object",
      "properties": {
        "expression": {
          "type": "string",
          "description": "The expression to evaluate in JavaScript."
        }
      },
      "required": ["expression"]
    },
    "testValue": "1.08933674"
  }
]
</Functions>


<User>
what is 2 to the .12345 power?
</User>
```

If you run this Promptfile, you will get the following output. Notice the automatically generated `<Function>{:prompt}` which includes our `testValue`. In the Playground, another request is made after the function returns to get the next `<Assistant>{:prompt}` response.

```prompt
... rest of the prompt ...


<User >
what is 2 to the .12345 power?
</User>


<Assistant type="function_call">
{
  "name": "calculator",
  "arguments": "{\n  \"expression\": \"2 ** 0.12345\"\n}"
}
</Assistant>


<Function name="calculator">
"1.08933674"
</Function>


<Assistant >
2 to the power of 0.12345 is approximately 1.08933674.
</Assistant>
```


