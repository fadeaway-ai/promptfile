import { Steps } from 'nextra-theme-docs'
import { Callout } from 'nextra-theme-docs'

# Cookbook (Examples)

The Promptfile Cookbook provides a collection of practical examples that demonstrate Promptfile' capabilities.

<Steps>

### **Declaring Variables**


To declare variables for interpolation within your prompt, utilize the `<Code>{:prompt}` block. Use the `@{variableName}` syntax to use those variables in your prompt. Ensure that all variables are placed within a block. If you want the user to define the variable when interacting, do not declare it. The Playground will automatically as the user to define those undeclared variables.

```prompt copy filename="haiku.prompt"
<Code>
const myFirstVariable = "The deep blue ocean"
</Code>

<System>
You are HaikuGPT. Always respond to the user in the form of a haiku. In your response, add new lines to format the haiku.
</System>

<User>
@{myFirstVariable}
</User>

<Request model="gpt-3.5-turbo" />
```

### **Executing Code**


The `<Code>{:prompt}` block provides versatility, not only permitting variable declaration but also enabling code execution for a range of purposes, including accessing user profiles. The example below showcases how incorporating user profile data into your prompt yields a more personalized and engaging conversational interaction.

```prompt copy filename="executeCode.prompt"
<Code>
const userProfile = {
  name: 'Jane Doe',
  age: 25,
  location: 'Philadelphia, PA',
  occupation: 'Zoologist',
  hobbies: ['juggling', 'hiking', 'basket weaving'],
  favoriteColor: 'blue',
  favoriteFood: 'tacos',
}
const input = "What is the name of the user and what is their favorite food?"
</Code>

<System>
You are having a conversation with the following user:

###
@{JSON.stringify(userProfile)}
###
</System>

<User>
@{input}
</User>

<Request model="gpt-3.5-turbo" />
```

### **Loading data into your prompt**

You may also retrieve information from external sources, such as a database or website. In this example, we demonstrate fetching HTML content from a website and incorporating it into our prompt using an `<Code>{:prompt}` block. To integrate more dynamic information into the conversation, consider making API requests or loading external data.

Sample user queries might include: "Where did Elliott go to grad school?" or "Where has Elliott worked?".

```prompt copy filename="websiteChat.prompt"
<Code>
const res = await fetch('https://elliottburris.com')
const html = await res.text()
const input = "Where did Elliott go to school?"
</Code>


<System>
Your job is to answer questions based on the following website.
###
@{html}
###
</System>

<User>
@{input}
</User>

<Request model="gpt-3.5-turbo" />
```

### **Calling functions**

Employ the `<Tool>{:prompt}` block to create a tool (or function) that enhances the model's ability to perform tasks. In this case, a calculator is provided as a resource for the LLM to use in accomplishing its task.

Define the tool by specifying its parameters, which include the name of the tool, a description of the tool, the schema the LLM needs to return to execute the function, and the intended function to run. In this example, we use [Zod](https://zod.dev/) to parse and validate the data.

Please note that, as of now, only `gpt-3.5-turbo-0613` and `gpt-4-0613` models have been fine-tuned to effectively call functions.

```prompt
<Tool
  name="calculator"
  description="Calculates the result of a math expression."
  schema={z.object({ expression: z.string().describe('expression to eval with JavaScript') })}
  run={({ expression }) => '' + eval(expression)}
/>

<User>
What is 2 to the .12345 power?
</User>

<Request model="gpt-3.5-turbo-0613" />
```
### **Chaining two prompts together**

For complex prompts, achieving the desired outcome often requires executing multiple requests. In this example, we will link two prompts to generate both a synopsis of a fictitious play and a review of that fictitious play in a single prompt.


<Callout type="info"> Observe the utilization of the `<Test>{:prompt}` block, replacing the `<Code>{:prompt}` block. This creates a test block that passes the test case as arguments when running the file. However, when implementing the Promptfile file within the actual project, the arguments can be supplied as parameters during invocation. </Callout>

```prompt copy filename="promptChaining.prompt"
<Test>
const title = 'The Engineer Who Built Promptfile'
</Test>

<User>
You are a highly-skilled playwright. Below, I will provide the title of a fictional play. Please respond with a full synopsis of the play based on my title.

Title: "@{title}"
</User>

<Request model="gpt-3.5-turbo" />

<User>
You are now a play critic from the New York Times. Given the play synopsis you provided above, please respond with a review of the play.
</User>

<Request model="gpt-4" />
```

### **Managing State of Chat**

Leverage the `useState` hook to introduce state management in your chat. This enables tracking features such as the total number of messages or the most recent message sent by the user.

In this example, we set an empty string as the last message sent by the user and initialize the number of messages sent as 0. Subsequently, each time the user sends a message, we increment the message count and update the user's most recent message.

The `<State>{:prompt}` is generated automatically to a file to keep track of the state of a chat. Additionally, observe the onResponse attribute, which permits code execution based on the LLM request's response. In this situation, we use the LLM request's response to update the last message sent by the user and count the number of messages exchanged.

```prompt copy filename="state.prompt"
<Code>
const [lastMessageText, setLastMessageText] = useState('')
const [numMessages, setNumMessages] = useState(0)
const input = "Tell me a joke about dogs"
setNumMessages(numMessages + 1)
</Code>

<System>
You are FunnyGPT. Respond to the user with a message that's maximally likely to make them laugh.
</System>

<User>
@{input}
</User>

<Request model="gpt-3.5-turbo" onResponse={response => setLastMessageText(response.message)} />
```
### **Classifying Comments as Questions or Complaints**

LLMs excel in comprehending semantic meaning and text classification. This example highlights the `<For>{:prompt}` block, which enables iteration through an array of examples. Here, we iterate over an array of comments and their corresponding classifications.

```prompt copy filename="classify.prompt"
<Code>
const examples = [
  {
    message: 'ur product never works, def want my money back',
    classification: 'NEEDS_ATTENTION',
  },
  {
    message: 'Love it! This saves so much time',
    classification: 'SKIP',
  },
  {
    message: 'Does the product come in silver?',
    classification: 'NEEDS_ATTENTION',
  },
  {
    message: "would highly recommend not buying this product, I've had so many problems",
    classification: 'NEEDS_ATTENTION',
  },
]
const userMessage = 'This product is highly defunct. I want my money back!'
</Code>

<System>
You are an AI community moderator. Your job is to read and process comments on a social media post to see if the comments have a question or complaint. If the comment has a question or complaint, return "NEEDS_ATTENTION". Otherwise, return "SKIP". You are provided examples to help identify comments that are a question or complaint.
</System>

<For each={examples} as="example">
<User>
@{example.message}
</User>

<Assistant>
@{example.classification}
</Assistant>
</For>

<User>
@{userMessage}
</User>

<Request model="gpt-3.5-turbo" />
```

### **Using conditional logic**

In this example, conditional logic allows for dynamic adjustments to your prompt based on specific conditions. Utilizing the `useState hook`, we maintain a record of the number of messages sent. Subsequently, the `if` attribute is employed to modify the prompt contingent upon the message count.

```prompt copy filename="conditionalLogic.prompt"
<Code>
const [numMessages, setNumMessages] = useState(0)

const input = "What a day, I want you to tell me about something."
if (numMessages > 2){
  isLongConversation = true;
} else {
  isLongConversation = false;
}
</Code>

<System if={!isLongConversation}>
You are NickelbackGPT. In all of your responses to the user, make a brief reference to Nickelback. Only discuss things related to Nickelback.
</System>

<System if={isLongConversation}>
You are TaylorSwiftGPT. In all of your responses to the user, make a brief reference to Taylor Swift. Only discuss things related to Taylor Swift.
</System>

<User>
@{input}
</User>

<Request model="gpt-3.5-turbo" onResponse={() => setNumMessages(numMessages + 1)} />
```

</Steps>

