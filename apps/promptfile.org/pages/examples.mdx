import { Steps } from 'nextra-theme-docs'
import { Callout } from 'nextra-theme-docs'


# Cookbook (Examples)

The Promptfile Cookbook provides a collection of practical examples that demonstrate Promptfile's capabilities. We recommend copy/pasting these into VS Code so you can run them yourself. We added comments to help you understand what's happening.

<Steps>

### **Introduction**

```prompt
Welcome to Promptfile! Promptfile is a Markdown-like file format for working with language models. To help you learn Promptfile, we'll work through a few examples together and build some LLM applications. We'll start simple by using the `gpt-3.5-turbo` model from OpenAI to generate haikus.

This prompt contains two chat blocks: a System block and a User block. The System block (`<System>`) gives initial instructions to the language model, setting the scene. The User block (`<User>`) represents the user's input or query to the language model.

If you're unfamilar with chat blocks in the context of prompting, you can learn more from docs provided by OpenAI (https://platform.openai.com/docs/guides/gpt/chat-completions-api) and Anthropic (https://console.anthropic.com/docs/prompt-design).

To start, try executing this prompt by using the `Promptfile: run in playground` command. You can find this command by opening the command palette (Cmd+Shift+P on Mac, Ctrl+Shift+P on Windows) and searching for `Promptfile`.

You can also execute this command via the keyboard shortcut Cmd+Enter (Mac) / Ctrl+Enter (Windows).


<System>
You are HaikuGPT. Always respond to the user in the form of a haiku.
</System>


Feel free to change this text to anything you want!
<User>
Tell me about the ocean.
</User>
```
### **Including variables in your template**

You can also include variables in your template using the `@{variableHere}{:prompt}`. This allows you to have a more dynamic prompt. When using your prompt in your project, you can pass these variables in as arguments. Here you can also see how we define what model we are going to use and the arguments for our request in the frontmatter.

```prompt
---
model: gpt-3.5-turbo
maxTokens: 50
temperature: 0.5
---

Awesome, you're well on your way to working with Promptfile! Promptfiles also support variables â€” you can establish a variable using the `@{` and `}` syntax. Let's try it out!

<System>
You are a New York Times movie critic. The User will provide a movie title that they would like for you to review. You task is to provide a review of the movie in @{maxNumberOfWords} words or less. If you are unfamiliar with the movie the User provides, you may ask the User for a different movie title.
</System>


<User>
Movie title: "@{movieTitle}"
</User>

You'll notice that when you run this file, you're first asked to fill in variables for `maxNumberOfWords` and `movieTitle`. Those values will be inserted into the prompt before the LLM is queried.
```

### **Try different models and adjust model settings**

Not only do we support OpenAI models, we support Anthropic. You can change which model you are using by changing the frontmatter where you can also edit the model setttings. Here in this example, we can see we use the `claude-v1` model. We provide diagnostics to help you make sure your request is valid regardless of what model you are using. For example, if you were to try to use a `<System>{:prompt}`block in an Anthropic request, we would notify you that Anthropic models don't support the `<System>{:prompt}`block. Or for example, if you were to try to make a request to OpenAI with a temperature of 100 which would not make a valid request, we'd provide a diagnostic letting you know of that.

```prompt
---
model: claude-v1
---

<User>
Tell me a joke about dogs.
</User>
```

Our goal is to help you run the file and test it out, so for certain situations such as when you use a `<System>{:prompt}`block in an Anthropic request, we automatically convert that to a `<User>{:prompt}` block so you don't have to go in and change your entire prompt.


### **Calling functions**

Employ the `<Functions>{:prompt}` block to define a function that enhances the model's ability to perform tasks. If your function returns something, it will be wrapped inside a `<Function>{:prompt}` block. For the purposes of the demo example, we include a `testValue` that our function would return.

Please note that only `gpt-3.5-turbo-0613` and `gpt-4-0613` models have been fine-tuned to call functions.

```prompt
---
model: gpt-3.5-turbo-0613
---

<Functions>
[
  {
    "name": "calculator",
    "description": "Calculates the result of a math expression using JavaScript `eval`.",
    "parameters": {
      "type": "object",
      "properties": {
        "expression": {
          "type": "string",
          "description": "The expression to evaluate in JavaScript."
        }
      },
      "required": ["expression"]
    },
    "testValue": "1.08933674"
  }
]
</Functions>

<User>
what is 2 to the .12345 power?
</User>
```

</Steps>