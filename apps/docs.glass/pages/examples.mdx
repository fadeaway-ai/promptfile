import { Steps } from 'nextra-theme-docs'
import { Callout } from 'nextra-theme-docs'

# Cookbook (Examples)

The Glass Cookbook includes examples to help you get started with Glass. You can also find examples in the demo [folder](https://github.com/glass-lang/glass/tree/main/apps/demo/examples).

<Steps>

### Add Variables to your Prompt

Here you can declare variables that you can interpolate into your prompt. You can declare variables in the `<Init>{:glass}` block and then use them in your prompt with the `@{}{:glass}` syntax.

```glass copy filename="haiku.glass"
<Init>
const myFirstVariable = "Hello, World!"
</Init>

<System>
You are HaikuGPT. Always respond to the user in the form of a haiku.
</System>

<User>
@{myFirstVariable}
</User>

<Request model="gpt-3.5-turbo" />
```

### Access Code in Your Prompt

Using a `<Init>{:glass}` block, you can execute code to do things like access a user's profile. In this example, you can see how you could use a user profile in our prompt to make the conversation more personal.

```glass copy filename="executeCode.glass"
<Init>
const userProfile = {
  name: 'Jane Doe',
  age: 25,
  location: 'Philadelphia, PA',
  occupation: 'Zoologist',
  hobbies: ['juggling', 'hiking', 'basket weaving'],
  favoriteColor: 'blue',
  favoriteFood: 'tacos',
}
const input = "What is the name of the user and what is their favorite food?"
</Init>

<System>
You are having a conversation with the following User:

###
@{JSON.stringify(userProfile)}
###

</System>

<User>
@{input}
</User>

<Request model="gpt-3.5-turbo" />
```

### Fetch information using code

You could also fetch information from somewhere else, like your database or a website.In this example, we'll fetch the HTML from a website and use it in our prompt. Ask it something like: "Where did Elliott go to grad school?" or "Where has Elliott worked?"

```glass copy filename="websiteChat.glass"
<Init>
const res = await fetch('https://elliottburris.com')
const html = await res.text()
const input = "Where did Elliott go to school?"
</Init>


<System>
Your job is to answer questions based on the following website.

###
@{html}
###
</System>


<User>
@{input}
</User>

<Request model="gpt-3.5-turbo" />
```

### Add a state to your chat

Using the `useState` hook, you can add state to your chat. This allows you to keep track of things like the number of messages sent or the last message sent by the user.

In this case, we initialize an empty string as the last message sent by the user and 0 as the number of messages sent. Then, we increment the number of messages sent each time the user sends a message and update the last message sent by the user.

The `<State>{:glass}`
You can also see the `onResponse` attribute which lets you execute code based on the response from the LLM request. In this case, we update the last message sent by the user with the response from the LLM request.

```glass copy filename="state.glass"
<Init>
const [lastMessageText, setLastMessageText] = useState('')
const [numMessages, setNumMessages] = useState(0)
const input = "Tell me a joke about dogs"
setNumMessages(numMessages + 1)
</Init>

<System>
You are FunnyGPT. Respond to the user with a message that's maximally likely to make them laugh.
</System>

<User>
@{input}
</User>

<Request model="gpt-3.5-turbo" onResponse={response => setLastMessageText(response.message)} />
```

### Chain two prompts together

For more complicated prompts, you often need the ability to make multiple requests to achieve your goal. In this example, we'll chain two prompts together to create a play synopsis and a review of the play.

```glass copy filename="promptChaining.glass"
<Init>
const title = 'The Glass Experience: Into the AIverse'
</Init>

<User>
You are a highly-skilled playwright. Below, I will provide the title of a fictional play. Please respond with a full synopsis of the play based on my title.

Title: "@{title}"
</User>

<Request model="gpt-3.5-turbo" />

<User>
You are now a play critic from the New York Times. Given the play synopsis you provided above, please respond with a review of the play.
</User>

<Request model="gpt-4" />
```

### Classify Comments as Questions or Complaints

LLMs can also be great at understanding semantic meaning and classifying text. In this example, we see the usage of the `<For>{:glass}` block which allows you to iterate through an array of examples. In this case, we are iterating through an array of comments and their classifications. We then use the `examples` variable in our prompt to help classify the user's comment.

```glass copy filename="classify.glass"
<Init>
const examples = [
  {
    message: 'ur product never works, def want my money back',
    classification: 'NEEDS_ATTENTION',
  },
  {
    message: 'Love it! This saves so much time',
    classification: 'SKIP',
  },
  {
    message: 'Does the product come in silver?',
    classification: 'NEEDS_ATTENTION',
  },
  {
    message: "would highly recommend not buying this product, I've had so many problems",
    classification: 'NEEDS_ATTENTION',
  },
]
const userMessage = 'This product is highly defunct. I want my money back!'
</Init>

<System>
You are an AI community moderator. Your job is to read and process comments on a social media post to see if the comments have a question or complaint. If the comment has a question or complaint, return "NEEDS_ATTENTION". Otherwise, return "SKIP". You are provided examples to help identify comments that are a question or complaint.
</System>

<For each={examples} as="example">
<User>
@{example.message}
</User>

<Assistant>
@{example.classification}
</Assistant>
</For>

<User>
@{userMessage}
</User>

<Request model="gpt-3.5-turbo" />
```

### Use conditional logic in your prompt

In this example, you can use conditional logic to change what goes into your prompt based on a condition. In this case, we are using the `useState` hook to keep track of the number of messages sent. Then, we use the `if` attribute to change the prompt based on the number of messages sent.

```glass copy filename="conditionalLogic.glass"
<Init>
const [numMessages, setNumMessages] = useState(0)
const iinput = "What a day, I want you to tell me about something."
</Init>

<System if={!isLongConversation}>
You are NickelbackGPT. In all of your responses to the user, make a brief reference to Nickelback. Only discuss things related to Nickelback.
</System>

<System if={isLongConversation}>
You are TaylorSwiftGPT. In all of your responses to the user, make a brief reference to Taylor Swift. Only discuss things related to Taylor Swift.
</System>

<User>
@{input}
</User>

<Request model="gpt-3.5-turbo" onResponse={() => setNumMessages(numMessages + 1)} />
```

</Steps>
